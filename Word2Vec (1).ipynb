{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fn3x47otkQJO",
        "outputId": "70cf118a-0436-4b9c-f48b-426f728e19ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li4bfES_jAys",
        "outputId": "2962a4a5-73fd-4899-aaf8-b60cdc682f7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.090895355\n",
            "[('want', 0.20281259715557098), ('a', 0.20181255042552948), ('is', 0.15810854732990265), ('class', 0.1072460189461708), ('to', 0.09825789928436279), ('show', 0.09279508143663406), ('object', 0.09159383922815323), ('person', 0.0913870632648468), ('illustration', 0.08460120111703873), ('point', 0.07196750491857529)]\n",
            "0.007032015\n",
            "[('such', 0.18382108211517334), ('is', 0.17312681674957275), ('world', 0.16813184320926666)]\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import warnings\n",
        " \n",
        "warnings.filterwarnings(action = 'ignore')\n",
        " \n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        " \n",
        "#  Reads ‘alice.txt’ file\n",
        "s = \"I like to work in UCP and UCP is a world class university. PU is also world class university.The illustration examples paragraph is useful when we want to explain or clarify something, such as an object, a person, a concept, or a situation. When we illustrate, we show how something is as we point out.\"\n",
        "#s = sample.read()\n",
        " \n",
        "# Replaces escape character with space\n",
        "f = s.replace(\"\\n\", \" \")\n",
        " \n",
        "data = []\n",
        " \n",
        "# iterate through each sentence in the file\n",
        "for i in sent_tokenize(f):\n",
        "    temp = []\n",
        "     \n",
        "    # tokenize the sentence into words\n",
        "    for j in word_tokenize(i):\n",
        "        temp.append(j.lower())\n",
        " \n",
        "    data.append(temp)\n",
        " \n",
        "# Create CBOW model\n",
        "model1 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, window = 5)\n",
        " \n",
        "# Print results\n",
        "print(model1.wv.similarity('like', 'class'))\n",
        "#model1.wv.most_similar(positive=['woman', 'king'], negative=['man'])\n",
        "p=model1.wv.most_similar(positive=['ucp', 'world'], negative=['pu'])\n",
        "print(p)\n",
        "\n",
        "# Create Skip-Gram model\n",
        "model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, window = 5, sg = 1)\n",
        "# Print results\n",
        "print(model2.wv.similarity('examples', 'class'))\n",
        "sims = model2.wv.most_similar('class', topn=3)\n",
        "print(sims)\n"
      ]
    }
  ]
}